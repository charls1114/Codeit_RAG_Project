import os
import sys
from pathlib import Path
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

# FastAPI ê´€ë ¨ ì„í¬íŠ¸
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn

# --- ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from src.rag_service.tracing import setup_tracing
from src.rag_service.pipelines.ingest import ingest_documents
from src.rag_service.pipelines.qa_chain import build_rag_chain
from src.rag_service.config import get_app_config

# [ì¶”ê°€] DB ì§ì ‘ ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# ì „ì—­ ë³€ìˆ˜
rag_chain = None
retriever_for_check = None  # [ì¶”ê°€] ë°ì´í„°ê°€ ìˆëŠ”ì§€ ë¯¸ë¦¬ ì°”ëŸ¬ë³¼ ê²€ìƒ‰ê¸°
chat_history = []  # [ì¶”ê°€] ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (íœ˜ë°œì„±)

# =================================================================
# ğŸ“ ê²½ë¡œ ì„¤ì •
# =================================================================
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
static_dir = project_root / "static"

# ê¶Œí•œ ë¬¸ì œë¡œ ìˆ˜ì • ë¶ˆê°€ëŠ¥í•œ ë°ì´í„° ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
data_dir = Path("/home/public/data")
raw_data_path = data_dir / "raw_data"
# =================================================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    global rag_chain, retriever_for_check
    print("ì„œë²„ ì‹œì‘ ì¤‘... ì„¤ì • ë° ëª¨ë¸ ë¡œë”©...")

    cfg = get_app_config()
    chroma_db_path = cfg.vectorstore.persist_dir
    setup_tracing()

    # 1. ë¬¸ì„œ ì ì¬ ì‹œë„ (ê¶Œí•œ ì—†ìœ¼ë©´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ try-except ê°ì‹¸ê¸°)
    if not os.path.exists(chroma_db_path) or not os.listdir(chroma_db_path):
        print("âš ï¸ DBê°€ ë¹„ì–´ìˆì–´ ë³´ì…ë‹ˆë‹¤. ì ì¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            if raw_data_path.exists():
                ingest_documents(raw_data_path)
            else:
                print(f"âŒ ë°ì´í„° í´ë” ì—†ìŒ: {raw_data_path}")
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ì ì¬ ì¤‘ ì—ëŸ¬ ë°œìƒ (ê¶Œí•œ ë¬¸ì œ ë“±): {e}")
            print("ğŸ‘‰ ê¸°ì¡´ DBë¥¼ ì½ê¸° ì „ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ë¹ˆ ìƒíƒœë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 2. [í•µì‹¬] ê²€ìƒ‰ê¸°(Retriever) ë³„ë„ ìƒì„±
    # RAG ì²´ì¸ê³¼ ë³„ê°œë¡œ, 'ë¬¸ì„œê°€ ì§„ì§œ ìˆë‚˜?' í™•ì¸ìš©ìœ¼ë¡œ ì”ë‹ˆë‹¤.
    try:
        embedding_function = OpenAIEmbeddings(model=cfg.embeddings.model_name)
        vectorstore = Chroma(
            persist_directory=chroma_db_path,
            embedding_function=embedding_function,
            collection_name=cfg.vectorstore.collection_name,
        )
        # ê²€ìƒ‰ê¸° ìƒì„± (ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜)
        retriever_for_check = vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": 1, "score_threshold": 0.3},  # ì •í™•ë„ 0.3 ë¯¸ë§Œì´ë©´ ë¬´ì‹œ
        )
        print("âœ… ë°ì´í„° í™•ì¸ìš© ê²€ìƒ‰ê¸°(Retriever) ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # 3. ì²´ì¸ ìƒì„±
    rag_chain = build_rag_chain(
        k_text=cfg.retrieval.k_text,
        k_table=cfg.retrieval.k_table,
        k_image=cfg.retrieval.k_image,
    )
    print("ğŸš€ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

    yield
    print("ì„œë²„ ì¢…ë£Œ.")


app = FastAPI(lifespan=lifespan)
app.mount("/static", StaticFiles(directory=static_dir), name="static")


class QuestionRequest(BaseModel):
    query: str


@app.get("/", response_class=HTMLResponse)
async def read_root():
    index_file = static_dir / "index.html"
    if not index_file.exists():
        return HTMLResponse(content="<h1>Error: index.html not found</h1>", status_code=404)
    with open(index_file, "r", encoding="utf-8") as f:
        return f.read()


@app.post("/api/chat")
async def chat_endpoint(request: QuestionRequest):
    global rag_chain, retriever_for_check, chat_history

    user_query = request.query

    # ---------------------------------------------------------
    # 1ë‹¨ê³„: ì°¸ê³ ìë£Œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (Pre-check)
    # ---------------------------------------------------------
    if retriever_for_check:
        # DBì—ì„œ ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì„œ 1ê°œë¥¼ ì°¾ì•„ë´…ë‹ˆë‹¤.
        docs = retriever_for_check.invoke(user_query)

        # ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì•ˆ ì¡íˆë©´ ë°”ë¡œ ê±°ì ˆ ë©”ì‹œì§€ ë¦¬í„´
        if not docs:
            print(f"ğŸ“­ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{user_query}'")
            return {"answer": "ì°¸ê³ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    # ---------------------------------------------------------
    # 2ë‹¨ê³„: ë©”ëª¨ë¦¬(ì´ì „ ëŒ€í™”) ì ìš©
    # ---------------------------------------------------------
    # ìµœê·¼ ëŒ€í™” 2í„´(ì§ˆë¬¸+ë‹µë³€ 2ì„¸íŠ¸)ë§Œ ìš”ì•½í•´ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. (ìš©ëŸ‰ ì ˆì•½)
    recent_history = chat_history[-4:]
    history_text = "\n".join(recent_history)

    # ì§ˆë¬¸ì„ [ì´ì „ ëŒ€í™” ìš”ì•½ + í˜„ì¬ ì§ˆë¬¸] í˜•íƒœë¡œ ìˆ˜ì •í•´ì„œ AIì—ê²Œ ë˜ì§‘ë‹ˆë‹¤.
    augmented_query = f"""
    [ì´ì „ ëŒ€í™” ë‚´ìš© ì°¸ê³ ]
    {history_text}

    [í˜„ì¬ ì§ˆë¬¸]
    {user_query}
    """

    # (ë””ë²„ê¹…ìš©) ì‹¤ì œë¡œ AIì—ê²Œ ë“¤ì–´ê°€ëŠ” ì§ˆë¬¸ ì¶œë ¥
    print(f"ğŸ“ ì…ë ¥ í”„ë¡¬í”„íŠ¸:\n{augmented_query}")

    # ---------------------------------------------------------
    # 3ë‹¨ê³„: RAG ë‹µë³€ ìƒì„±
    # ---------------------------------------------------------
    if rag_chain is None:
        return {"answer": "ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    # ìˆ˜ì •ëœ ì§ˆë¬¸(augmented_query)ì„ ë„£ìŠµë‹ˆë‹¤.
    # ë§Œì•½ AIê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ìŠëŠ”ë‹¤ë©´ request.queryë¥¼ ê·¸ëŒ€ë¡œ ì“°ë˜,
    # ë¬¸ë§¥ ìœ ì§€ê°€ ì•ˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬ ë°©ì‹ì´ ê°€ì¥ í˜¸í™˜ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.)
    answer = rag_chain.invoke(augmented_query)
    final_answer = str(answer)

    # ---------------------------------------------------------
    # 4ë‹¨ê³„: ë©”ëª¨ë¦¬ì— ì €ì¥ (íœ˜ë°œì„±)
    # ---------------------------------------------------------
    chat_history.append(f"Q: {user_query}")
    chat_history.append(f"A: {final_answer}")

    # ë©”ëª¨ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì•ì—ì„œë¶€í„° ìë¦„ (ìµœëŒ€ 10ê°œ ë¬¸ì¥ ìœ ì§€)
    if len(chat_history) > 10:
        chat_history.pop(0)

    return {"answer": final_answer}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8005)
